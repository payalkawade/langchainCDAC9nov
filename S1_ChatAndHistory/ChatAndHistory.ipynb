{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and the forecast for the rest of the day. \\n\\n- Have a great day' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '149077b7-61e7-438a-a90e-ee95888bde87', 'token_count': {'input_tokens': 232.0, 'output_tokens': 44.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '149077b7-61e7-438a-a90e-ee95888bde87', 'token_count': {'input_tokens': 232.0, 'output_tokens': 44.0}} id='run-85115712-8285-4aee-b525-37d642de737d-0' usage_metadata={'input_tokens': 232, 'output_tokens': 44, 'total_tokens': 276}\n",
      "content=\"According to our latest forecast, today is looking mostly sunny with a high of 72°F (22°C) and a low of 50°F (10°C). There's a gentle breeze blowing at about 5 mph (8 km/h) from the west. It's a beautiful day to get outside and enjoy the sunshine!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 51, 'total_tokens': 123, 'completion_time': 0.06, 'prompt_time': 0.009352604, 'queue_time': 0.003812455999999999, 'total_time': 0.069352604}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-38f09fcb-fc91-4c6d-99ea-a6d00c63dc09-0' usage_metadata={'input_tokens': 51, 'output_tokens': 72, 'total_tokens': 123}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0b4901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"Top Gun\" is an action-packed thriller set in the world of the elite pilots of the United States Navy. The film follows the story of Lieutenant Pete \"Maverick\" Mitchell (Tom Cruise), a young and daring pilot with a need for speed and a reputation for testing his limits.\\n\\nThe movie opens with Maverick and his radar intercept officer (RIO), Lieutenant Nick \"Goose\" Bradshaw (Anthony Edwards), participating in a training exercise at the Top Gun Naval Flying School. Maverick\\'s skill and recklessness are on full display as he pushes his aircraft to the limits, impressing and alarming his peers and superiors.\\n\\nAt Top Gun, Maverick competes with the best of the best, including his rival, Tom \"Iceman\" Kazansky (Val Kilmer). As the training intensifies, Maverick struggles to balance his competitive nature with the importance of teamwork and following orders. He also finds romance with his civilian instructor, Charlotte \"Charlie\" Blackwood (Kelly McGillis), who helps him gain a deeper understanding of flight tactics.\\n\\nHowever, tragedy strikes when Goose dies in a training accident, causing Maverick to question his own abilities and the risks he takes. Struggling with self-doubt and guilt, Maverick considers quitting the program, but with the support of his friends and Charlie, he decides to stay and honor Goose\\'s memory.\\n\\nIn the climax of the film, Maverick and his new RIO, Lieutenant Leonard \"Wolfman\" Wolfe (Barry Tubb), are sent on a mission to intercept hostile enemy aircraft over the Indian Ocean. Despite the odds, Maverick\\'s skill and bravery shine as he leads his team to victory, proving his worth as a top pilot.\\n\\nThroughout the movie, \"Top Gun\" explores themes of friendship, loss, and the challenges of military life. It showcases the intense training and camaraderie of the elite pilots, as well as the personal sacrifices they make in the line of duty. The film also highlights the technological prowess of military aviation, with breathtaking aerial sequences and state-of-the-art aircraft.\\n\\nIn the end, Maverick emerges as a true hero, having overcome his personal demons and proven his skill, dedication, and bravery. \"Top Gun\" became a cultural phenomenon, launching Tom Cruise into superstardom and inspiring a generation of aspiring pilots. Its blend of thrilling action, memorable characters, and heart-pounding soundtrack continues to captivate audiences even decades later.' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'e9f6a69d-f78a-486d-9e6b-549efd688e75', 'token_count': {'input_tokens': 214.0, 'output_tokens': 493.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'e9f6a69d-f78a-486d-9e6b-549efd688e75', 'token_count': {'input_tokens': 214.0, 'output_tokens': 493.0}} id='run-a51df954-d6fe-430e-a055-c27158b31ff2-0' usage_metadata={'input_tokens': 214, 'output_tokens': 493, 'total_tokens': 707}\n",
      "content='Here\\'s a summary of the classic 1986 film \"Top Gun\":\\n\\n**Top Gun (1986)**\\n\\n**Tagline:** \"I feel the need, the need for speed.\"\\n\\n**Synopsis:** Maverick (Tom Cruise) is a hotshot fighter pilot in the United States Navy\\'s elite fighter weapons school, better known as Top Gun. Despite his reckless behavior and disregard for authority, Maverick is a natural-born pilot with exceptional skills.\\n\\nUpon arrival at Top Gun, Maverick is paired with Radar Intercept Officer (RIO) Goose (Anthony Edwards), a by-the-book pilot with a no-nonsense approach. The two pilots quickly become rivals with fellow Top Gun students Iceman (Val Kilmer) and Slider (Rick Rossovich).\\n\\nAs the competition heats up, Maverick catches the eye of his instructor, Charlie (Kelly McGillis), a civilian instructor and a former flight instructor who is also a military pilot. Their romance blossoms, but it\\'s complicated by Maverick\\'s fear of commitment and his need for freedom.\\n\\nMeanwhile, Top Gun is rocked by the sudden death of one of its pilots, which serves as a wake-up call for the students and instructors. Maverick and Goose\\'s rivalry turns into a deep friendship, and they work together to overcome their personal demons and become better pilots.\\n\\nIn the thrilling final showdown, Maverick and his RIO, Goose, face off against the Soviet Union\\'s best pilots in a high-stakes dogfight. Maverick\\'s skills and instincts are put to the test as he engages in a series of daring aerial stunts and outmaneuvers his opponents.\\n\\n**Themes:** The film explores themes of male bonding, rivalry, love, and self-discovery, set against the backdrop of high-stakes combat and intense aerial action.\\n\\n**Notable scenes:** The film features several iconic scenes, including the aerial dogfights, the memorable \"I feel the need, the need for speed\" line, and the dramatic conclusion.\\n\\n**Impact:** \"Top Gun\" became a cultural phenomenon, grossing over $356 million worldwide and cementing Tom Cruise\\'s status as a Hollywood star. The film\\'s success also spawned a sequel, \"Top Gun: Maverick,\" which was released in 2022.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 456, 'prompt_tokens': 28, 'total_tokens': 484, 'completion_time': 0.38, 'prompt_time': 0.00505164, 'queue_time': 0.009641029, 'total_time': 0.38505164}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-8c262f0d-d079-48fe-ba2b-d2457e540b14-0' usage_metadata={'input_tokens': 28, 'output_tokens': 456, 'total_tokens': 484}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='you are movie summary createor'),\n",
    "    HumanMessage(content='tell me summary of topgun movie')\n",
    "]\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x76a719bbd490> async_client=<cohere.client.AsyncClient object at 0x76a723f2b020> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/home/codespace/.python/current/lib/python312.zip', '/home/codespace/.python/current/lib/python3.12', '/home/codespace/.python/current/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/home/codespace/.python/current/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The woman walks the last 10 flights of stairs because she hates taking the stairs, but she hates walking in the rain even more.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('walk10Flights') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={}), HumanMessage(content='guess, what is my favourite color', additional_kwargs={}, response_metadata={})]\n",
      "A guessing game!\n",
      "\n",
      "Hmm... I'm going to take a wild guess... Is your favorite color... BLUE?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "store.add_message(HumanMessage('guess, what is my favourite color'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d71a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "You're testing me, Bob! I'm glad to know that your name is indeed Bob. I'm happy to chat with you, Bob! What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob! Nice to meet you! Is there something I can help you with or would you like to chat?\n",
      "You've just introduced yourself as Bob! I've got it down, dude!\n"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba5eb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Payal! How are you doing today?\n",
      "Ha! I just guessed it correctly. But don't worry, I'm a language model, I'm designed to learn and remember names. I'll try to remember yours, Payal, and use it in our conversation if I can.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am payal\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
